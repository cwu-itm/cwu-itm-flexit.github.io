<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <link href="css/styles.css" rel="stylesheet">
      <title>Joseph Robi | Portfolio</title>
   </head>
   <body>
<!-- Page Header  -->
      <header class="page-header">
           <div class="header-inner">
               <a class="logo" href="#">DEV/BLOG</a>
               <nav class="main-nav" aria-label="Primary">
                  <a href="#">Technology</a>
                  <a href="#">Culture</a>
                  <a href="#">Lifestyle</a>
                  <a href="#">People</a>
                  <a href="#">About</a>
                  <a class="nav-cta" href="#">Subscribe</a>
               </nav>
            </div>
      </header>
<!-- Main Content Area  -->
      <main>
         <!-- Module 1.1 Blog Post Starts Here-->
         <article>
            <header class="article-header">
               <h2>Module 1.1 Blog Post</h2>
            </header>
            <h3>Model-Based Systems Thinking</h3>
               <p>As digital systems continue to grow in scale and complexity, traditional approaches to design and problem solving aren't as effective as they once were. Modern systems are no longer isolated pieces of software or hardware; instead they are interconnected networks of components, stakeholders, constraints, and environments that continuously interact with one another. Cloud platforms, enterprise applications, transportation technologies, and large-scale information systems all demonstrate how difficult it is to predict outcomes when many elements operate together. In this context, systems thinking provides a way to understand complexity more effectively, while model-based systems engineering (MBSE) offers a practical method for managing it.</p>
               <p>This paper defines systems thinking in my own words, contrasts it with more traditional linear ways of thinking, and explains why a model-based approach is particularly well suited for designing and managing complex digital systems. Rather than treating systems as collections of independent parts, systems thinking and MBSE focus on relationships, interactions, and the broader context in which systems exist.</p>
            <h3>Defining Systems Thinking</h3>
               <p>Systems thinking can be defined as an approach to understanding systems by focusing on how components interact with one another within a defined boundary and how those interactions produce behavior over time. Rather than treating a system as a collection of independent parts, systems thinking emphasizes the relationships, dependencies, and feedback loops that connect those parts and shape overall system behavior. This approach encourages looking beyond individual components to identify patterns that influence how the system functions as a whole. From this perspective, the behavior of a system cannot always be fully explained by analyzing its parts separately, because many outcomes emerge from the way elements interact with one another within the broader context of the system.</p>
               <p>A key idea behind systems thinking is that systems exist within other systems. No system operates entirely on its own. Holt explains that the elements of a system are best understood in relation to each other and to other systems rather than in isolation (Holt & Weilkiens, 2023). This idea is especially relevant in digital environments, where software systems depend on infrastructure, external services, users, and organizational processes. A change made to one part of the system may lead to unexpected consequences elsewhere, even if the change seems minor.</p>
               <p>Another important aspect of systems thinking is the concept of boundaries. A system boundary defines what is considered part of the system and what lies outside it. Boundaries help establish scope, but they are not always fixed or universally agreed upon. Different stakeholders may perceive the boundary differently depending on their role and interests (Holt & Weilkiens, 2023). Systems thinking requires acknowledging these differences and making boundary decisions explicit so that assumptions are shared rather than implied. Overall, systems thinking is less about finding quick solutions and more about developing an accurate understanding of how a system behaves, why problems occur, and how changes may affect the system over time.</p>
            <h3>Traditional Linear Thinking</h3>
               <p>Traditional linear thinking approaches problems by breaking them down into smaller parts and addressing each part independently. This approach assumes clear and direct cause-and-effect relationships, where a specific action leads to a predictable and measurable outcome. Linear thinking works well in situations where systems are relatively simple, stable, and well understood, and where interactions between components are limited (Interaction Design Foundation, 2016). Many engineering tasks, classroom examples, and academic exercises rely on this approach because it is efficient, structured, and easy to manage.</p>
               <p>However, linear thinking struggles when applied to complex systems. In these environments, cause and effect are often separated by time, distance, or multiple layers of interaction, making outcomes harder to predict (Interaction Design Foundation, 2016). A change made today may not reveal its full impact until much later, and the final outcome may be shaped by factors that fall outside the original scope of the problem. Linear thinking tends to overlook these indirect effects because it focuses narrowly on individual components or immediate results rather than broader system behavior.</p>
               <p>Another limitation of linear thinking is its tendency toward local optimization. Teams may improve one part of a system without fully considering how that improvement affects the rest of the system (Interaction Design Foundation, 2016). For example, increasing processing speed in one service may create bottlenecks in another, or improving security controls may unintentionally reduce usability for end users. Without a broader view of the system and its interactions, well-intentioned changes can introduce new problems that were not anticipated during design.</p>
            <h3>How Systems Thinking Differs from Linear Thinking</h3>
               <p>Systems thinking differs from linear thinking in several fundamental ways. First, systems thinking prioritizes interactions over individual components. While linear thinking asks how a part functions on its own, systems thinking asks how that part interacts with other elements and contributes to overall system behavior. Holt emphasizes that interactions between system elements are just as important as the elements themselves (Holt & Weilkiens, 2023).</p>
               <p>Second, systems thinking recognizes that system behavior is often emergent. Emergent behavior refers to outcomes that arise from interactions among components rather than from any single component acting alone. In complex digital systems, issues including cascading failures, performance degradation, or security vulnerabilities often emerge from combinations of small decisions rather than a single obvious flaw.</p>
               <p>Third, systems thinking is inherently stakeholder-focused. Different stakeholders interact with the system in different ways and have different needs, goals, and perspectives. Holt describes stakeholders as roles rather than specific individuals, noting that a single person may occupy multiple stakeholder roles depending on the context (Holt & Weilkiens, 2023). Systems thinking encourages designers to consider these diverse perspectives early, reducing the risk of conflicts or unmet needs later in the system's life cycle. Finally, systems thinking is adaptive rather than static. It acknowledges that systems evolve over time and that understanding must be continuously updated as conditions change. This makes systems thinking particularly well suited to digital environments, where technologies, regulations, and user expectations change rapidly.</p>
            <h3>The Need for a Model-Based Approach</h3>
               <p>As systems grow more complex, relying solely on informal descriptions, disconnected documents, or individual expertise becomes risky. Model-based systems engineering addresses this challenge by using formal models to represent system structure, behavior, requirements, constraints, and relationships. These models serve as a shared source of truth that supports communication, analysis, and decision making.</p>
               <p>The applicability of MBSE becomes clear when considering the core elements of systems thinking. Holt discusses system elements, interfaces, stakeholders, boundaries, needs, and constraints as fundamental concepts in systems engineering (Holt & Weilkiens, 2023). MBSE provides a way to capture these concepts explicitly rather than leaving them implicit or scattered across multiple documents. For complex digital systems, this explicit representation is critical. Digital systems often involve numerous interfaces, both internal and external. Data flows, control signals, authentication mechanisms, and integration points must all work together reliably. A model-based approach allows teams to define and analyze these interfaces systematically, reducing the likelihood of misunderstandings or integration failures.</p>
            <h3>MBSE and Managing Complexity</h3>
               <p>One of the greatest advantages of MBSE is its ability to make complexity manageable. Rather than overwhelming teams with excessive detail, a well-designed model highlights what matters most at each level of abstraction (Singam & Carter, 2025). High-level models can focus on system goals and stakeholder needs, while more detailed models can address architecture, behavior, and interfaces. MBSE also supports consistency across different viewpoints. Complex systems are typically examined from multiple perspectives, such as functional, operational, technical, and organizational (Singam & Carter, 2025). Without a model-based approach, these perspectives are often documented separately, leading to inconsistencies. MBSE allows these views to be derived from a single underlying model, ensuring alignment across teams.</p>
               <p>Change management is another area where MBSE is particularly valuable. Digital systems are rarely static, and changes are inevitable. When requirements, constraints, or technologies change, a model can help identify which parts of the system are affected (Meißner et al., 2021). This makes it easier to assess impacts before changes are implemented, reducing the risk of unintended consequences.</p>
            <h3>Boundaries, Interfaces, and Stakeholders in MBSE</h3>
               <p>Boundaries play a central role in both systems thinking and MBSE. Defining system boundaries helps clarify scope, ownership, and responsibility. In digital systems, boundaries are often conceptual rather than physical, which can make them difficult to define clearly. MBSE supports boundary definition by explicitly modeling what is inside the system and what lies outside it. Interfaces occur wherever interactions cross system boundaries. Holt notes that identifying interfaces is essential for specifying and defining systems (Holt & Weilkiens, 2023). In digital environments, poorly defined interfaces are a common source of failure. MBSE helps teams document and analyze interfaces early, improving integration and reducing ambiguity.</p>
               <p>Stakeholders also benefit from a model-based approach. Because stakeholders view systems differently depending on their role, models can provide tailored views that address specific concerns without losing alignment with the overall system. This supports clearer communication and more informed decision making throughout the system life cycle.</p>
            <h3>MBSE as a Support Tool, Not a Replacement for Judgment</h3>
               <p>While MBSE offers many benefits, it is important to recognize its limitations. Models are simplifications of reality and must be maintained to remain useful. Poorly designed or outdated models can create false confidence rather than clarity. Holt emphasizes that systems engineering does not eliminate the need for intelligence and judgment, and practitioners should not blindly follow prescribed methods (Holt & Weilkiens, 2023). The value of MBSE is in how it is used. When applied thoughtfully it enhances understanding, communication, and analysis. When applied from a more mechanical perspective it can become a bureaucratic exercise. Effective use of MBSE requires selecting the right level of detail and focusing on aspects of the system that are most critical to success.</p>
            <h3>Conclusion</h3>
               <p>Systems thinking provides a framework for understanding complex systems by emphasizing relationships, interactions, boundaries, and stakeholder perspectives. Unlike traditional linear thinking, which assumes simple cause-and-effect relationships, systems thinking acknowledges that behavior often emerges from the interaction of many components over time. This perspective is essential for understanding modern digital systems, where complexity and interdependence are the norm.</p>
               <p>Model-based systems engineering complements systems thinking by offering a structured way to represent and manage complexity. By making system elements, interfaces, needs, constraints, and boundaries explicit, MBSE supports better communication, more informed decision making, and improved change management. Grounded in the core principles of systems engineering, a model-based approach helps teams design and evolve complex digital systems more effectively while remaining adaptable in a constantly changing environment.</p>
         <footer class="article-footer">
            <h3>Sources:</h3>
            <ol>
               <li>
                  Holt, J., &amp; Weilkiens, T. (2023).
                  <em>Systems engineering demystified: Apply modern, model-based systems engineering techniques to build complex systems</em> (2nd ed.).
                  Packt Publishing.
               </li>
               <li>
                  Interaction Design Foundation. (2016, November 26).
                  <em>What is linear thinking?</em>
                  <a href="https://www.interaction-design.org/literature/topics/linear-thinking">
                     https://www.interaction-design.org/literature/topics/linear-thinking
                  </a>
               </li>
               <li>
                  Singam, C., &amp; Carter, J. (2025, November 17).
                  <em>Model based systems engineering (MBSE)</em>.
                  Systems Engineering Body of Knowledge.
                  <a href="https://sebokwiki.org/wiki/Model-Based_Systems_Engineering_%28MBSE%29">
                     https://sebokwiki.org/wiki/Model-Based_Systems_Engineering_%28MBSE%29
                  </a>
               </li>
               <li>
                  Meißner, M., Jacobs, G., Jagla, P., &amp; Sprehe, J. (2021).
                  <em>Model based systems engineering as enabler for rapid engineering change management</em>.
                  <em>Procedia CIRP, 100</em>, 146-151.
                  <a href="https://doi.org/10.1016/j.procir.2021.05.010">
                     https://doi.org/10.1016/j.procir.2021.05.010
                  </a>
               </li>
            </ol>
         </footer>
         </article>

         <!-- Module 1.2 Blog Post Starts here -->
         <article>
            <header class="article-header">
               <h2>Module 1.2 Blog Post</h2>
            </header>
            <h3>Understanding the Systems Development Life Cycle</h3>
               <p>Digital systems do not get built once and then quietly “exist.” They launch, get used, patched, scaled, extended, and eventually replaced as conditions around them change. Even when a system feels stable from the outside, it is still being shaped by evolving user needs, emerging security threats, growing data volumes, shifting regulations, and rising expectations around performance, availability, and reliability. That constant pressure is why systems engineers emphasize life cycles. Life cycle thinking is not just a project management tool. It is a long-term mindset for designing systems that can endure real-world complexity long after development ends.</p>
               <p>In systems engineering terms, a life cycle describes how a system evolves through stages, while a life cycle model describes how those stages are executed and ordered (Holt & Weilkiens, 2023). This distinction matters because many projects share similar stages but follow very different execution patterns depending on uncertainty, risk, organizational constraints, and technical complexity. In this post, I define the systems development life cycle, describe the key characteristics, advantages, and drawbacks of linear, iterative, and incremental life cycle models, and reflect on why life cycle thinking is essential for the design, sustainability, and long-term evolution of modern digital systems.</p>
            <h3>Defining the Systems Development Life Cycle</h3>
               <p>The systems development life cycle, often called the SDLC, is the full progression a system goes through from its initial idea to its retirement. It is not limited to building and deploying software (Awati & Gillis, 2024). Instead, it captures the complete evolution of a system as it is conceived, developed, introduced, used, supported, and eventually replaced. When people describe a system as “done,” they are usually referring to a temporary milestone, such as a release or deployment. From a systems perspective, that moment is only a transition, not an ending. A system continues to evolve as it interacts with real users and operating environments (Holt & Weilkiens, 2023).</p>
               <p>Most digital systems move through similar life cycle phases. While terminology differs, the underlying pattern is consistent. Conception is where needs are defined. Stakeholders identify the problem, clarify goals, surface constraints, and determine what success looks like. In digital environments, this includes decisions about users, data, security, compliance, and performance expectations. Development is where potential solutions are explored and refined into a workable design and architecture. This includes decisions about components, interfaces, integrations, workflows, and how the system will be tested and deployed (Awati & Gillis, 2024). Even when “design” and “development” are separated, they still answer the same question: what are we building, and how should it behave?</p>
               <p>Production is where the system is constructed and validated. For digital systems, this often includes infrastructure setup, environment configuration, and monitoring, not just writing code (Holt & Weilkiens, 2023). The goal is to ensure the system works correctly and satisfies the original needs. Utilization is when the system begins serving real users. This is where assumptions are tested and real usage patterns appear. Data volumes grow, edge cases surface, and dependencies change. The system becomes part of a larger ecosystem. Support includes maintenance, security updates, performance tuning, monitoring, and incident response. For most digital systems, this is the longest phase and the one that determines long-term reliability and trust.</p>
               <p>Retirement is the planned decommissioning of the system. This often involves data migration, regulatory retention, shutting down integrations, and transitioning users (Holt & Weilkiens, 2023). When retirement is ignored, organizations become stuck with fragile legacy systems. These phases demonstrate why systems are not “finished” after launch. Real environments change continuously, and systems must evolve to remain useful and secure.</p>
            <h3>Life Cycle Models</h3>
               <p>A life cycle defines the stages a system passes through. A life cycle model describes how those stages are executed and ordered (Holt & Weilkiens, 2023). Linear, iterative, and incremental models represent different approaches to handling uncertainty and change.</p>
                  <h4>The Linear Life Cycle Model</h4>
                     <p>The linear model executes stages in a fixed, sequential order. Each phase is completed before the next begins, and progress moves forward in a straight line from conception to delivery. The classic example of this approach is the waterfall model, which has been widely used in both engineering and software development contexts. Its main advantage is clarity. Linear models are easy to plan, communicate, and govern because each stage has a clearly defined start and end point. They work best when requirements are stable, technologies are well understood, and the overall system scope is limited. In these situations, teams can move confidently from one stage to the next without needing to revisit earlier decisions.</p>
                     <p>However, the linear model assumes a level of certainty that many digital systems do not have. When requirements change late in the process, linear execution absorbs the cost through extensive rework. Another drawback is late feedback. Usability issues, performance limitations, or design flaws may remain hidden until deployment, when fixes are most expensive and disruptive. Holt and Weilkiens note that linear models are best suited to small, well-defined projects and not to large, complex systems where needs change frequently (Holt & Weilkiens, 2023).</p>
                  <h4>The Iterative Life Cycle Model</h4>
                     <p>The iterative model is built around repetition and continuous learning. Instead of a single pass through the stages, the team cycles through them multiple times, using each iteration to improve the system based on feedback and new insights. Each cycle produces a version of the system that can be evaluated, tested, and refined. Its greatest strength is feedback (Efimova, 2024). Iteration reduces the risk of building the wrong system because assumptions are tested early and often. It also supports adaptation, allowing the system to evolve as user needs, technologies, and expectations change. Iterative approaches are especially useful for complex digital systems where requirements are uncertain or still emerging. They align well with prototyping, usability testing, and continuous improvement practices that are common in modern digital environments.</p>
                     <p>The drawback is the need for discipline. Without clear goals, boundaries, and quality controls, iteration can become chaotic. Teams may rush releases, overlook long-term impacts, and accumulate technical debt. Holt and Weilkiens emphasize that model-based methods can still be applied in iterative environments to help manage complexity and improve communication among stakeholders (Holt & Weilkiens, 2023).</p>
                  <h4>The Incremental Life Cycle Model</h4>
                     <p>The incremental model delivers a system in pieces rather than all at once. Instead of a single final release, the system grows through multiple increments that add new functionality and capabilities over time. Each increment represents a step toward the fully realized system. Its primary advantage is early value. Users and organizations can begin benefiting from the system sooner, and overall project risk is reduced by introducing capability gradually rather than through a single large release.</p>
                     <p>Incremental approaches work well for long projects and for systems that can be decomposed into meaningful components or modules. They allow organizations to avoid disruptive “big bang” deployments and instead introduce change in a controlled and manageable way. However, not all systems can be broken into useful subsets. Integration across increments can also become complex and difficult to manage. Holt and Weilkiens note that incremental models are effective when partial capability is useful early, but unsuitable when the system cannot be meaningfully decomposed (Holt & Weilkiens, 2023).</p>
            <h3>Comparing the Models</h3> 
               <p>The key difference among these models is how they treat change. Linear approaches resist change by locking decisions early and assuming that most requirements can be defined in advance. Iterative approaches expect change and use repeated cycles to absorb new information, feedback, and shifting needs as the system evolves. Incremental approaches also expect growth, but they focus on delivering that growth in clearly defined stages through controlled releases that gradually expand system capability (Efimova, 2024).</p>
               <p>Risk is also managed differently across the three models. Linear models rely heavily on upfront planning and documentation, which can work well when uncertainty is low but can fail when assumptions prove incorrect. Iterative models reduce risk through early learning, frequent validation, and continuous refinement, allowing problems to surface before they become costly (Efimova, 2024). Incremental models reduce rollout risk by limiting the scope of each release, although they introduce their own challenges related to coordination, integration, and long-term consistency.</p>
               <p>Fit ultimately depends on context. Stable, small projects may suit linear execution because the requirements are unlikely to change and the scope is well understood. User-facing digital systems often benefit from iteration because user expectations, technologies, and competitive pressures evolve rapidly (Efimova, 2024). Large enterprise systems may benefit from incremental delivery to reduce disruption and allow organizations to adopt new capabilities gradually. In practice, many organizations blend these models. The goal is not to follow one model rigidly, but to choose an execution pattern that aligns with system complexity, organizational constraints, and the level of uncertainty involved.</p>  
            <h3>Life Cycle Thinking</h3>
               <p>Life cycle thinking matters because digital systems must survive long after launch. Security threats evolve, dependencies change, and compliance requirements continue to grow as regulations and industry standards shift. Systems that cannot be updated safely or consistently over time quickly become liabilities rather than assets. Scaling is another major challenge. As user bases expand, growth affects performance, data volume, reliability, and the complexity of support processes. These pressures usually emerge after deployment, not during initial development, which is why planning only for launch is never sufficient. User feedback also reshapes systems in important ways. Real usage reveals design gaps, unexpected behaviors, and limitations that are impossible to fully predict in advance (Holt & Weilkiens, 2023). Without a life cycle mindset that embraces ongoing change, systems become rigid, difficult to adapt, and increasingly irrelevant to users.</p>
               <p>Sustainability depends heavily on maintainability. Systems that are difficult to understand, modify, or extend drain organizational resources and slow innovation. Ignoring life cycle planning leads directly to technical debt, where short-term decisions turn into long-term burdens that limit future flexibility. Digital systems also depend on external technologies that have their own life cycles. When those technologies decline, become obsolete, or change unexpectedly, systems must adapt or risk failure. Life cycle thinking helps teams anticipate and manage these transitions rather than simply reacting to them when problems arise (Holt & Weilkiens, 2023).</p>
            <h3>Conclusion</h3>
               <p>The systems development life cycle describes how digital systems evolve from conception through development, deployment, utilization, support, and retirement. Life cycle models define how those stages are executed. The linear model offers simplicity but struggles with change. The iterative model supports learning and adaptation but requires discipline. The incremental model enables staged value delivery but depends on strong architecture and integration. For modern digital systems, life cycle thinking is essential. It allows teams to design systems that evolve rather than collapse under change. Systems that succeed over time are not the ones that launch once, but the ones built to grow, adapt, and endure.</p>
         <footer class="article-footer">
            <h3>Sources:</h3>
            <ol>
               <li>
                  Holt, J., &amp; Weilkiens, T. (2023).
                  <em>Systems engineering demystified: Apply modern, model-based systems engineering techniques to build complex systems</em> (2nd ed.).
                  Packt Publishing.
               </li>
               <li>
                  Awati, R., &amp; Gillis, A. S. (2024, September 23).
                  <em>What is systems development life cycle?</em>
                  TechTarget.
                  <a href="https://www.techtarget.com/searchsoftwarequality/definition/systems-development-life-cycle">
                     https://www.techtarget.com/searchsoftwarequality/definition/systems-development-life-cycle
                  </a>
               </li>
               <li>
                  Efimova, D. (2024, July 12).
                  <em>Comparison of SDLC models: How to choose the best for your project?</em>
                  EPAM.
                  <a href="https://startups.epam.com/blog/software-development-models-comparison">
                     https://startups.epam.com/blog/software-development-models-comparison
                  </a>
               </li>
            </ol>
         </footer>
         </article>
         <!-- Module 2.1 Blog Post Starts here -->
         <article>
            <header class="article-header">
               <h2>Module 2.1 Blog Post</h2>
            </header>
            <h3>Designing for People Means Designing for Minds</h3>
               <p>“Design for people” sounds like a slogan until you sit with what it actually implies. People do not interact with interfaces as purely neutral or perfectly rational users. They arrive with prior experiences, limited attention, imperfect memory, and goals that are shaped by stress, time pressure, and context. They also arrive with expectations about how things should work, because every product they've used before quietly trains them. Human-centered design treats these realities as core requirements rather than edge cases.</p>
               <p>That mindset is why I think human-centered design is less about making something look friendly and more about making something fit the way humans perceive, decide, and act. If the design fights perception, it will be overlooked. If it fights attention, it will be ignored. If it fights memory, it will become an afterthought. If it fights empathy, it will feel like the system was built for the organization instead of the person using it.</p>
            <h3>Human-Centered Design</h3>
               <p>Human-centered design is often described as “putting users first,” but that framing can be misleading because it can sound like a preference instead of a method. In practice, it is a structured approach to developing interactive systems by grounding decisions in an understanding of users, tasks, and environments, involving users directly, and iterating designs based on feedback until the solution reliably meets user requirements (International Organization for Standardization [ISO], 2019). That last point matters. Human-centered design is not simply listening to users once, then building whatever the team prefers. It is an ongoing loop where evidence continues to shape the product.</p>
               <p>The implication here with human-centered design is that usability and user experience do not “appear” at the end. They're outcomes of choices made earlier, especially choices about what problems the system is truly solving, for who, and under what real-world constraints. In other words, designing for people starts long before the UI, because the UI is only the visible layer of a bigger relationship between human goals and system behavior.</p>
            <h3>ISO 9241-210 and IDEO's Field Guide</h3>
               <p>ISO 9241-210 and IDEO's Field Guide both land on the same commitment: meaningful design requires a real understanding of the people affected by it. Where they differ is in how they frame the work and what they emphasize as “good practice.”</p>
               <p>ISO 9241-210 reads more like a professional standard (because it technically is one) and treats human-centered design as a process that can be planned, integrated into different development approaches, and executed with clear stages. The emphasis is on comprehensive understanding of context of use, formalizing user requirements so they are auditable, evaluating designs against those requirements, and iterating until the design solutions meet them (ISO, 2019). It also stresses that human-centered design is not the job of one role. It assumes interdisciplinary competencies and repeatedly brings the user or domain expert perspective into the project as an active duty, not a nice-to-have (ISO, 2019). That framing makes it easier to defend human-centered work in environments that demand traceability, accountability, and risk management.</p>
               <p>IDEO's Field Guide, by contrast, feels like a field manual for creative problem solving. Its language centers on belief, mindset, and practice: the conviction that problems are solvable, the habit of learning directly from people, the willingness to prototype, the expectation of early failure, and the continuous shift between diverging into many options and converging toward what is most desirable, feasible, and viable (IDEO.org, 2015). It outlines phases like Inspiration, Ideation, and Implementation, but it is honest that real projects are not linear and that the imperfect is normal (IDEO.org, 2015). ISO more so provides structure you can map to governance. IDEO more so provides momentum you can map to discovery.</p>
               <p>Taken together, they describe human-centered design as both disciplined and exploratory. ISO keeps teams anchored to context, requirements, and evaluation. IDEO keeps teams willing to learn, iterate, and make ideas tangible so that feedback can shape reality, not just theory. If ISO helps you prove that you designed responsibly, IDEO helps you keep designing when you do not yet know the answer.</p>
            <h3>Perception and Visual Hierarchy</h3>
               <p>A major reason human-centered design works is that it respects the difference between what designers intend users to see and what users actually notice. Perception is selective. Attention is limited. Visual hierarchy is the tool we use to guide both. Consider a simple example such as a checkout page with a large promo-code box placed above the “Place Order” button. The design may be technically correct, but perception will often treat that promo box as the next required step. Users pause, scan, and wonder whether they missed something. Some leave to search for a code. Others assume they cannot proceed without one. That is not a content problem. It is a hierarchy problem. The interface accidentally told users that discounts are the primary task, not completing the purchase.</p>
               <p>The same dynamic shows up in dashboards and business tools. If a screen presents ten equal-weight charts, equal-weight colors, and equal-weight typography, it effectively tells the user, “All of this is equally urgent.” But users do not have equal attention to give. They need the interface to do some of the sorting. Strong hierarchy does that by making the primary action visually dominant, by grouping related information, and by reducing competition for attention. When hierarchy is weak, users compensate by reading more, scanning longer, and making more guesses. That increases cognitive load and produces more errors, even if every element is technically accessible.</p>
               <p>This is where “designing for people” becomes concrete. People use shortcuts. They look for shapes and patterns. They trust what is prominent. They assume that what is familiar will behave in familiar ways. If your hierarchy is misaligned with user goals, your usability issues will not show up as broken features. They will show up as hesitation, repeated scanning, and abandonment.</p>
            <h3>How the Four UX Laws Support Human-Centered Decisions</h3>
               <p>The four UX laws often get presented as design trivia, but they are more useful when treated as explanations for why certain choices consistently succeed or fail. They connect perception, attention, and memory to measurable outcomes.</p>
               <p>Jakob's Law is the expectation engine. Users spend most of their time in other products, so they bring learned patterns into your interface (Chung, 2023). Designing for people means leveraging those patterns rather than demanding that users relearn basics. For example, if your navigation behaves differently from common conventions, the cost is not only confusion. The cost is attention. Users shift from goal-focused behavior to interface-focused behavior, and that mental mode switch is exhausting (Chung, 2023). Human-centered design does not eliminate innovation, but it chooses carefully where novelty is worth the learning burden and where familiarity is the more humane choice.</p>
               <p>Fitts's Law explains why usability is physical as well as cognitive. People do not just decide what to click (Budiu, 2022). They also have to acquire the target. Small, tightly packed buttons, especially near other dangerous actions, increase movement time and error rate. A common mistake is treating spacing and size as purely aesthetic. In reality, they shape effort and accuracy. If you want people to take the primary action, you make it easier to hit, not just easier to understand. If you want to prevent irreversible mistakes, you create separation and require intentionality. That is empathy expressed through geometry.</p>
               <p>Hick's Law is the attention tax. The more choices presented at once, the longer it takes to decide, especially when options are similar or poorly differentiated (Interaction Design Foundation, 2016). This shows up everywhere in digital systems: settings menus with dozens of toggles, onboarding flows that ask users to choose a plan before they understand value, or enterprise forms that present every field as mandatory up front. A human-centered approach respects that decision-making is work. It reduces the work by staging complexity, using progressive disclosure, and helping users recognize the best next step rather than forcing them to evaluate every possible step.</p>
               <p>Miller's Law, often summarized as limits on working memory, is the memory reality check (Uzegbu, 2024). People can only hold a small number of items in mind at once, and that capacity drops further when they are stressed, distracted, or unfamiliar with the domain. This is why chunking is not optional. It is why long, ungrouped forms feel overwhelming. It is why confirmation screens that repeat key details reduce anxiety. It is why showing a summary of what the user already selected prevents them from having to remember it. Designing for people means not treating memory as free storage. It means designing so that users can recognize information rather than recall it from scratch.</p>
               <p>When these laws are taken seriously, they stop being abstract. They become the rationale behind hierarchy, layout, interaction design, and even product scope. They also connect naturally to ISO and IDEO. ISO pushes you to ground decisions in user context and evaluate against requirements, which makes these laws testable. IDEO pushes you to prototype and learn quickly, which is how you discover where attention, memory, and expectation are breaking down in the real world.</p>
            <h3>Empathy, Human Factors and Real Outcomes</h3>
               <p>Empathy is sometimes framed as emotional warmth, but in design it is closer to disciplined perspective-taking. It is the habit of assuming that friction is more often a design problem than a user problem. It is also the habit of asking what conditions shape behavior. Is the user hurried? Are they on a small screen? Are they interrupted? Are they anxious about making an error? Those conditions change what “good usability” means.</p>
               <p>ISO emphasizes that desk research alone is insufficient, and that direct user involvement and feedback are required to iteratively refine solutions (ISO, 2019). IDEO frames empathy as a mindset and a practice, grounded in learning from people and making ideas tangible so feedback can improve them (IDEO.org, n.d.). Both perspectives resist a dangerous shortcut: designing based only on what the team assumes would be “reasonable.” People are not unreasonable. They are contextual. Human-centered design honors that context.</p>
               <p>In practical terms, empathy changes what teams measure and what they treat as success. It shifts focus from whether a feature exists to whether a feature supports a human goal with minimal friction. It shifts focus from whether a workflow is logically complete to whether it is cognitively manageable. It also encourages teams to define usability issues more honestly. If a user keeps making the same “mistake,” it is rarely because they are careless. It is often because the design is teaching the wrong lesson through hierarchy, labels, or interaction constraints.</p>
            <h3>What It Means to Design for People</h3>
               <p>Designing for people is designing with humility. It is accepting that perception, attention, and memory set hard constraints on usability, and that ignoring those constraints does not make them go away. It is also accepting that empathy is not a sentiment, it is a method. It shows up in user involvement, in iterative feedback loops, and in design choices that reduce effort, prevent errors, and respect how humans actually behave.</p>
               <p>ISO 9241-210 offers a durable foundation for doing this work systematically, especially in environments that need rigor and accountability. IDEO's Field Guide offers the creative posture needed to learn from people, prototype quickly, and stay flexible when the problem is not yet fully understood. The four UX laws give everyday design decisions a psychological backbone, explaining why familiar patterns matter, why target size matters, why choice overload matters, and why chunking matters.</p>
               <p>When those pieces come together, “design for people” stops being a nice idea and becomes a practical design standard: build from real context, guide attention with intentional hierarchy, reduce cognitive and physical effort, and keep learning from the humans on the other side of the screen.</p>
         <footer class="article-footer">
            <h3>Sources:</h3>
            <ol>
               <li>
                  International Organization for Standardization. (2019).
                  <em>ISO 9241-210:2019. Ergonomics of human-system interaction - Part 210: Human-centred design for interactive systems</em>.
                  <a href="https://www.iso.org/standard/77520.html">
                  https://www.iso.org/standard/77520.html
                  </a>
               </li>
               <li>
                  IDEO.org. (2015).
                  <em>The field guide to human-centered design</em>.
                  <a href="https://d1r3w4d5z5a88i.cloudfront.net/assets/guide/Field%20Guide%20to%20Human-Centered%20Design_IDEOorg_English-0f60d33bce6b870e7d80f9cc1642c8e7.pdf">
                  https://d1r3w4d5z5a88i.cloudfront.net/assets/guide/Field%20Guide%20to%20Human-Centered%20Design_IDEOorg_English.pdf
                  </a>
               </li>
               <li>
                  Chung, E. (2023, June 22).
                  <em>Jakob's Law: Creating familiar and user-centric interfaces</em>.
                  LogRocket.
                  <a href="https://blog.logrocket.com/ux-design/jakobs-law-creating-user-centric-interfaces">
                  https://blog.logrocket.com/ux-design/jakobs-law-creating-user-centric-interfaces
                  </a>
               </li>
               <li>
                  Budiu, R. (2022, July 31).
                  <em>Fitts's Law and its applications in UX</em>.
                  Nielsen Norman Group.
                  <a href="https://www.nngroup.com/articles/fitts-law/">
                  https://www.nngroup.com/articles/fitts-law/
                  </a>
               </li>
               <li>
                  Interaction Design Foundation. (2016, June 29).
                  <em>What is Hick's Law?</em>
                  <a href="https://www.interaction-design.org/literature/topics/hick-s-law">
                  https://www.interaction-design.org/literature/topics/hick-s-law
                  </a>
               </li>
               <li>
                  Uzegbu, C. (2024, November 26).
                  <em>How can Miller's Law make UX better?</em>
                  LogRocket.
                  <a href="https://blog.logrocket.com/ux-design/millers-law-ux-design">
                  https://blog.logrocket.com/ux-design/millers-law-ux-design
                  </a>
               </li>
            </ol>
         </footer>
         </article>
         <!-- Module 2.2 Blog Post Starts here -->
         <article>
            <header class="article-header">
               <h2>Module 2.2 Blog Post</h2>
            </header>
            <h3>What Makes Digital Systems Usable</h3>
               <p>Usability is often described as something a system either has or doesn't have, as if it was something that's added late in development. In practice, usability emerges from how well a digital system aligns with the way people think, remember, and act. Nowadays memory is limited and attention is selective. Therefore, habits and mental shortcuts guide behavior far more than conscious analysis. When systems are usable, it is not because users are exceptionally patient or skilled, but because the system respects those cognitive realities and works with them rather than against them.</p>
               <p>Designing usable systems therefore means designing for minds, not idealized users. It requires understanding how mental models shape expectations, how memory constraints influence interaction, and how consistency and feedback reduce cognitive effort. When systems ignore these factors, they force users to compensate by remembering too much, reinterpreting familiar actions, or decoding unnecessary complexity. Over time, that friction erodes trust and efficiency, even if the system technically works as intended.</p>
               <figure>
                  <img src="images/design.jpg" alt="Person writing and sketching ideas related to design and planning" >
                  <figcaption>
                     Photo by 
                     <a href="https://unsplash.com/@uxindo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                        UX Indonesia
                     </a> 
                     on 
                     <a href="https://unsplash.com/photos/person-writing-on-white-paper-qC2n6RQU4Vw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                        Unsplash
                     </a>.
                  </figcaption>
               </figure>
            <h3>Memory, Mental Models, and User Interaction</h3>
               <p>Memory plays a central role in how people interact with digital systems, yet it is often misunderstood. People do not approach interfaces as blank slates. They bring mental models shaped by prior experiences with similar systems. A mental model is not a detailed technical understanding of how a system works, but a rough, functional expectation of what actions are possible and what outcomes those actions will produce. When a system aligns with a user's mental model, interaction feels intuitive. When it violates that model, even simple tasks feel confusing.</p>
               <p>Human memory compounds this effect. Working memory, which supports conscious thought and decision-making, is limited and easily overloaded. Long-term memory, while more durable, is unreliable when systems demand arbitrary recall rather than recognition (Johnson, 2010). Usable systems reduce reliance on memory by making actions and information visible, predictable, and consistent. Poorly designed systems do the opposite. They require users to remember commands, terminology, or sequences that have little inherent meaning.</p>
               <p>This distinction becomes clear when comparing systems that rely on recognition versus recall. A navigation menu that clearly displays available options allows users to recognize what they need. A system that hides actions behind ambiguous icons or undocumented shortcuts forces users to remember what those symbols mean or to relearn them repeatedly. Over time, users adapt, but adaptation is not the same as usability. It simply means users are absorbing complexity that the system could have handled instead.</p>
            <h3>Designing with the Mind in Mind: Supporting Reading and Memory</h3>
               <figure>
                  <img src="images/digital system.jpg" alt="Computer screen displaying complex digital code" >
                  <figcaption>
                     Photo by 
                     <a href="https://unsplash.com/@hishahadat?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                        Shahadat Rahman
                     </a> 
                     on 
                     <a href="https://unsplash.com/photos/shallow-focus-photography-of-computer-codes-BfrQnKBulYQ?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                        Unsplash
                     </a>.
                  </figcaption>
               </figure>
               <p>Jeff Johnson's Designing with the Mind in Mind provides a clear lens for understanding how easily design choices can disrupt cognition. One core principle emphasized in the book is that poor information design can disrupt automatic reading and force users into slower, more effortful processing (Johnson, 2010). Skilled reading is largely automatic. When text uses familiar vocabulary, clear typography, and predictable layouts, readers process it with minimal conscious effort. When those conditions are violated, working memory is engaged unnecessarily, slowing comprehension and increasing error.</p>
               <p>Unfamiliar vocabulary is a common offender. Technical jargon such as “reauthenticate” may be precise from a developer's perspective, but it interrupts reading for nontechnical users who must stop and interpret its meaning. The same disruption occurs with centered text, tiny fonts, or low-contrast backgrounds. These choices force readers out of automatic reading mode and into deliberate decoding. The result is not just slower reading, but increased cognitive load that competes with the user's primary task (Johnson, 2010).</p>
               <p>A related principle from Johnson's work concerns long-term memory. People need tools to augment memory, not systems that burden it. Yet many digital systems do exactly that by requiring users to remember passwords, security answers, or procedural steps that offer no meaningful cues. Authentication systems are a classic example. When users are asked to create “easy to remember” credentials under restrictive rules, the system is effectively asking them to solve a memory problem that it created. Predictably, users respond by writing information down, reusing passwords, or relying on support channels. The usability cost is real, even if the security requirement was well intentioned (Johnson, 2010).</p>
               <p>Both principles point to the same conclusion: usable systems minimize the need for reading, remembering, and interpreting. They support recognition, provide cues, and externalize information so users do not have to hold it all in their heads.</p>
            <h3>Habits, Consistency, and Mental Shortcuts</h3>
               <figure>
                  <img src="images/user interaction.jpg" alt="People sitting together and interacting in conversation" >
                  <figcaption>
                     Photo by 
                     <a href="https://unsplash.com/@fourcolourblack?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                        Adam Wilson
                     </a> 
                     on 
                     <a href="https://unsplash.com/photos/three-peson-sitting-talking-to-each-other-mM7V59F5syw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                        Unsplash
                     </a>.
                  </figcaption>
               </figure>
               <p>Habits are mental shortcuts built through repetition. They allow users to act quickly without conscious effort. In digital systems, habits form around consistent layouts, predictable interactions, and stable terminology. When a system respects those habits, users feel efficient and confident. When it violates them, users hesitate, double-check actions, or make errors.</p>
               <p>Interface consistency is therefore not about aesthetic uniformity. It is about protecting users' mental shortcuts (Yablonski, 2020). A button that looks and behaves the same way across screens reinforces habit. A menu that appears in a consistent location reduces search time. Even small inconsistencies, such as changing labels for the same action, can disrupt flow by forcing users to re-evaluate what they already “know.”</p>
               <p>System feedback plays a complementary role, for instance, feedback tells users whether their action was received, what the system is doing, and what will happen next. Without feedback, users are left to infer system state, often by guessing or repeating actions. That guessing consumes attention and creates anxiety, especially in time-sensitive or high-stakes contexts. Clear feedback reduces cognitive load by closing the loop between intention and outcome.</p>
               <p>A familiar example is file uploading. A system that shows a progress indicator, confirms completion, and explains errors supports a user's mental model of cause and effect. A system that offers no feedback leaves users wondering whether the action worked, encouraging duplicate uploads or premature navigation away from the page. In both cases, the functionality may be identical. The difference in usability lies in how much cognitive effort the system demands from the user.</p>
            <h3>Tesler's Law and the Burden of Complexity</h3>
               <p>Tesler's Law, also known as the law of conservation of complexity, provides a useful framework for understanding usability decisions. As described in Laws of UX, the law states that every system has an inherent amount of complexity that cannot be eliminated, only shifted. The critical design question is where that complexity should live: in the system or in the user's head. Usable systems assume as much complexity as possible on behalf of the user. Modern email clients illustrate this well. Users do not manually specify sender addresses or remember full email strings for frequent contacts. The system pre-populates fields, suggests recipients, and corrects errors. The complexity still exists, but it has been absorbed by design and development effort rather than imposed on users (Yablonski, 2020).</p>
               <p>Checkout processes offer another example. Entering billing and shipping information is inherently complex, but systems can reduce user burden by reusing data, validating entries in real time, and offering payment services that abstract the entire process. From the user's perspective, the task becomes simple. Behind the scenes, the system has taken on significant complexity to make that simplicity possible.</p>
               <p>Tesler's Law highlights a common failure mode in poorly designed systems: prioritizing implementation convenience over user effort. When designers leave complexity exposed because it is easier to build that way, users pay the cost repeatedly. As Tesler famously argued, if millions of users waste time on complexity that could have been removed by a small amount of design effort, the system has failed its users (Schwarz, 2025).</p> 
            <h3>Poor Design and Unnecessary Cognitive Load</h3>
               <p>Examples of poor usability are often mundane rather than dramatic. They show up as extra steps, confusing labels, or layouts that require excessive reading. Over time, these small frictions accumulate into significant cognitive load.</p>
               <p>One common example comes from enterprise software settings screens. It is not unusual to see dozens of toggles presented at once, each with lengthy descriptions written in dense prose. Users must read carefully, remember prior choices, and infer interactions between options. This design assumes unlimited attention and memory. A more usable approach would stage complexity, grouping related settings and revealing advanced options only when needed. The underlying functionality remains the same, but the cognitive burden shifts away from the user (UserBit, 2025).</p>
               <p>Another example is error messaging. Messages that describe problems in system-centric terms force users to translate technical language into actionable steps. An error such as “session expired” or “invalid token” explains the system's state but not the user's path forward. Clear feedback reframes errors in human terms, explaining what happened and what to do next. This reduces both confusion and memory burden, because users do not have to infer meaning from unfamiliar terminology.</p>
               <p>Even typography and layout choices contribute to cognitive load. Dense blocks of centered text, low-contrast color schemes, or repetitive phrasing make scanning difficult and disrupt automatic reading. Users may not consciously articulate the problem, but they experience it as fatigue or frustration. As Johnson notes, when reading becomes work, the interface itself becomes an obstacle (Johnson, 2010).</p>
            <h3>Consistency, Feedback, and Learning Over Time</h3>
               <p>Usability is not only about first-time use. It is also about learning and retention over time. Consistent interfaces support long-term memory by reinforcing patterns. When actions behave the same way across contexts, users do not have to relearn them. This consistency benefits both novices and experienced users, allowing skills to transfer rather than reset with each new feature (Johnson, 2010).</p>
               <p>Feedback reinforces learning by confirming correct actions and gently correcting errors. A system that acknowledges success helps users build confidence in their mental model. A system that silently fails leaves users uncertain, undermining trust. Over time, users gravitate toward systems that feel predictable, even if those systems are not the most feature-rich. This dynamic aligns closely with Tesler's Law (Schwarz, 2025). When complexity is handled by the system through consistent behavior and clear feedback, users can focus their cognitive energy on their goals rather than on operating the interface itself. That focus is the essence of usability.</p>
            <h3>What Makes a System Truly Usable</h3>
               <p>Usable digital systems respect human limitations and leverage human strengths. They acknowledge that memory is fragile, habits are powerful, and mental shortcuts are essential for efficiency. They minimize unnecessary reading, avoid burdening recall, and present information in ways that support automatic processing. They use consistency to reinforce habits and feedback to maintain trust.</p>
               <p>The principles from Designing with the Mind in Mind remind us that even small design decisions can disrupt cognition if they ignore how people read and remember. Tesler's Law reminds us that complexity is inevitable, but suffering is optional. Designers choose where complexity lives, and that choice defines the user experience.</p>
               <p>Ultimately, usability is not about removing all difficulty. It is about ensuring that the difficulty users face is inherent to their goals, not imposed by the system. When a digital system is usable, it feels less like a tool that must be operated and more like an extension of the user's intent. That is not accidental. It is the result of deliberate design choices grounded in an understanding of how minds actually work.</p>
         <footer class="article-footer">
            <h3>Sources:</h3>
            <ol>
               <li>
                  Johnson, J. (2010).
                  <em>Designing with the mind in mind</em>.
                  Elsevier.
               </li>
               <li>
                  Yablonski, J. (2020).
                  <em>Laws of UX: Using psychology to design better products and services</em>.
                  O'Reilly Media.
               </li>
               <li>
                  Schwarz, D. (2025, March 12).
                  <em>Tesler's law: The danger of oversimplifying complex products</em>.
                  LogRocket.
                  <a href="https://blog.logrocket.com/ux-design/teslers-law-in-ux">
                  https://blog.logrocket.com/ux-design/teslers-law-in-ux
                  </a>
               </li>
               <li>
                  UserBit. (2025, February 18).
                  <em>Understanding cognitive load in UX</em>.
                  <a href="https://userbit.com/content/blog/cognitive-load-ux-terms">
                  https://userbit.com/content/blog/cognitive-load-ux-terms
                  </a>
               </li>
            </ol>
         </footer>
         </article>
         <!-- Module 2.3 Blog Post Starts here -->

         <!-- Module 3.1 Blog Post Starts here -->
         <article>
            <header class="article-header">
               <h2>Module 3.1 Blog Post</h2>
            </header>
            <section class="article-content">
            <h3>How Visual and Interaction Design Steer User Movement</h3>
               <p>Digital interfaces rarely “force” users to do anything. Most of the time, they nudge. They suggest where to look, what to do next, and what to ignore. That guidance happens through layout and through controls: where elements sit on the screen, how they are grouped, what looks clickable, what looks safe, and what looks final. When those cues are aligned with user goals, people move through a system with a sense of momentum. When they're not, users stall, second-guess, or drift into trial-and-error mode.</p>
               <p>That is why visual and interaction design are inseparable. The interface is not just a box that contains random features. It's the logic the user can see. Layout communicates priority. Controls communicate available actions. Feedback communicates whether the system understood the user and what state the system is in now. Even before a user reads a word, the interface has already shaped behavior by signaling what matters and what can be postponed.</p>
               <figure>
                  <img src="images/drawing.jpg" alt="Person sketching interface ideas and design concepts on paper" >
                  <figcaption>
                     Photo by 
                     <a href="https://unsplash.com/@dtravisphd?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                        David Travis
                     </a>
                     on
                     <a href="https://unsplash.com/photos/person-holding-pink-sticky-note-WC6MJ0kRzGw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                        Unsplash
                     </a>.
                  </figcaption>
               </figure>
            <h3>Layout and Control Design Shape Behavior</h3>
               <p>Interface layout is essentially a map, and users move based on what the map implies. People scan for structure first: headings, clusters, whitespace, and the strongest visual elements. If the layout is coherent, they can predict where to find what they need. If it is cluttered or inconsistent, they spend more time searching than doing.</p>
               <p>Control design is the second half of the story. Controls are the “verbs” of an interface. Buttons, toggles, dropdowns, and links are not just mechanics. They communicate commitment level and risk. A primary button signals, “This is the next step.” A destructive action signals, “Be careful.” A disabled control signals, “Not yet.” When controls are designed without clear hierarchy or when too many actions compete for attention, users slow down because the interface is asking them to decide what the system should have clarified.</p>
               <p>A familiar example is an account settings screen. If “Save,” “Cancel,” “Reset,” and “Delete Account” all appear as same-sized buttons, same color, same weight, the interface creates unnecessary danger. Users must read more closely, confirm more carefully, and build their own mental safety rails. On the flip side, if the layout groups “profile details” separately from “security,” and if destructive actions are visually separated and clearly labeled, users move with confidence because the interface is doing part of the thinking with them.</p>
               <p>This is also where terminology matters. When labels shift across screens, user movement becomes less efficient because people cannot rely on recognition. They are forced into interpretation mode. Johnson's point about consistent terminology is practical here: if users are looking for “Search” and the system calls it “Query” in one place, the user can miss it entirely because attention is limited and goal-focused (Johnson, 2010). Inconsistent naming disrupts the path through the interface because it breaks the user's ability to scan and match what they expect to what they see.</p>
            <h3>Aesthetic-Usability Effect and Why First Impressions Change Behavior</h3>
               <p>One of the most interesting UX realities is that users do not judge usability purely by performance. They judge it emotionally and instantly. The Aesthetic-Usability Effect describes how aesthetically pleasing design tends to be perceived as more usable, and users become more tolerant of minor friction when the interface looks refined (Yablonski, 2020). This matters because movement through an interface is not just navigation. It is a willingness to continue. When a screen looks balanced, readable, and intentional, users assume competence. That assumption essentially buys the system a little patience. People are more likely to keep going even if the flow has minor friction. But the reverse is also true. When the interface looks dated, cluttered, or inconsistent, users become suspicious. They hesitate earlier. They double-check more. They may abandon faster because the design signals that other issues are likely hiding underneath.</p>
               <p>I have felt this most clearly in two different “same task” situations: paying a bill in a modern mobile app versus paying a bill on an older portal designed like a spreadsheet. The modern app typically has strong spacing, obvious primary actions, and clear feedback after each step. Even when a page takes an extra second to load, it still feels trustworthy. The older portal still technically works, but it feels like it could fail at any moment, so I read every label and confirm every field. The “usability” difference there is partially functional, but it is also emotional. The aesthetic sets the cognitive posture the user adopts. The implication is that in one instance the user is completing a task, while in the other they are defending themselves from the interface.</p>
               <p>There is a design risk here worth acknowledging. A polished interface can mask usability issues and make teams miss problems during testing because users blame themselves less or tolerate more (Yablonski, 2020). Aesthetic appeal can be a cushion, but it should not be a substitute for logic. If the flow is flawed, the polish just delays the moment when friction becomes unavoidable.</p>
               <figure>
                  <img src="images/UX design.jpg" alt="Person working on UX design sketches and interface planning" >
                  <figcaption>
                     Photo by 
                     <a href="https://unsplash.com/@amayli?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                        Amélie Mourichon
                     </a>
                     on
                     <a href="https://unsplash.com/photos/person-holding-pen-near-paper-sv8oOQaUb-o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                        Unsplash
                     </a>.
                  </figcaption>
               </figure>
            <h3>Von Restorff Effect and Guiding Attention Without Noise</h3>
               <p>If the aesthetic-usability effect is about emotional posture, the Von Restorff Effect is about attention and memory. When multiple similar objects are present, the one that differs is most likely to be noticed and remembered (Yablonski, 2020). In interface design, this often shows up as the “one primary button,” the highlighted alert, or the emphasized next step. Used well, this effect is one of the simplest ways to guide users through a flow. If a page has many secondary options, but only one action should be taken next, you make that action visually distinct. Users do not need a paragraph of instruction. Their eyes will go to what stands out first. That is not manipulation. It is clarity.</p>
               <p>Used poorly, it becomes noise. Too many highlighted items compete, and the user loses the benefit of emphasis entirely. I have seen dashboards where everything is a bright color, every card has a badge, and every section has a call-to-action. The result is not engagement. It is paralysis. The interface is shouting in every direction, so users have to decide what to ignore. That decision itself is cognitive load.</p>
               <p>Von Restorff is also critical in error prevention. If a modal asks the user to confirm a destructive action, the destructive control should not look like the safe control. Distinctiveness can reduce accidental clicks by ensuring the risky action is visually unmistakable. That aligns with a broader learning principle: people learn faster and explore more when the system feels low-risk and recoverable (Johnson, 2010). If the design makes mistakes easy to make and hard to undo, users become cautious and stop exploring. Their movement through the interface becomes narrow and repetitive because they are protecting themselves.</p>
            <h3>Postel's Law and Interaction Logic That Respects Human Variation</h3>
               <p>In UX terms Postel's Law is about designing inputs and interactions that accommodate real human behavior instead of demanding perfect formatting or perfect sequences (Brodlo, 2023). People type phone numbers with parentheses, spaces, dashes, or no separators at all. People paste addresses with extra whitespace. People capitalize inconsistently. A system that rejects these variations forces users into a frustrating loop of trial-and-error.</p>
               <p>The reason Postel's Law matters for “movement” is that rigid systems create dead ends. Users reach a step, hit an error, and lose flow (Allman, 2011). They are no longer progressing toward a goal. They are negotiating with validation rules. The system has shifted attention away from the user's task and toward the system's preferences, which is the opposite of usability. A concrete frustration I have had repeatedly is with web forms that require a phone number in a specific format but do not show the format until after you submit. You enter the number the way you always do, the form throws an error, and only then does it reveal the expected structure. That is not just annoying. It is inefficient interaction logic. Better design accepts common formats, normalizes input behind the scenes, and only blocks truly invalid entries. The user keeps moving, and the system quietly handles the complexity.</p>
               <p>Postel's Law connects directly to the idea of consistent, task-focused terminology and conceptual models. When systems reflect how users think about tasks, users do not have to translate their intentions into “system language” (Brodlo, 2023). The more translation required, the more the interface interrupts movement. Good interaction logic reduces translation by anticipating variation and providing constraints in ways that feel supportive rather than punitive.</p>
            <h3>A Real Experience With Interface Logic: When the Flow Breaks</h3>
               <p>One of the most frustrating experiences I have had recently was trying to schedule an appointment through a healthcare portal. The visual design looked modern enough, but the interaction logic had some friction. The layout presented a clean calendar, but the controls did not match what the screen implied. I could select an appointment time, but the “Continue” button stayed disabled without explanation. There was no feedback indicating what was missing. I clicked around thinking I had mis-tapped, then I scrolled back up and down looking for a required field. Eventually I realized the portal wanted me to select a provider first, but that requirement was not placed near the calendar and was not visually emphasized. The system knew what step I had skipped, but it did not tell me.</p>
               <p>That is a feedback failure. It is also a hierarchy failure. The interface showed me the calendar first, so I assumed time selection was the primary action. But the system's real logic required a different sequence. The result was cognitive load that felt like confusion. I went from doing the task to debugging the interface. My movement through the flow became “search, guess, retry,” which is exactly what good UX prevents.</p>
               <p>Contrast that with a scheduling flow that works well, like booking a haircut through a service app like “theCut”. I've been using the app for years and from my experience, the layout matches the logic. Step one asks for the service type. Step two asks for the provider. Step three shows times, lastly payment. Each page has one clear next action, and the system provides immediate feedback if something is missing. If a time is unavailable, it is visibly disabled. If a selection is required, the UI communicates it before you attempt to proceed. That design supports learning because the system makes the path obvious and reduces risk. Users are not afraid to explore because mistakes are cheap and reversible (Johnson, 2010).</p>
               <p>This is where the UX laws stop being abstract. Aesthetic-usability influences whether the user trusts the system enough to keep going. Von Restorff influences whether the next action is obvious without extra reading. Postel's Law influences whether the system accommodates real input and keeps the user moving. And Johnson's emphasis on consistency and low-risk learning environments explains why some interfaces feel easy even when the tasks are inherently complex.</p>
               <figure>
                  <img src="images/pink sticky notes.jpg" alt="Pink sticky notes used to organize ideas and group information" >
                  <figcaption>
                     Photo by 
                     <a href="https://unsplash.com/@dtravisphd?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                        David Travis
                     </a>
                     on
                     <a href="https://unsplash.com/photos/person-holding-pink-sticky-note-WC6MJ0kRzGw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                        Unsplash
                     </a>.
                  </figcaption>
               </figure>
            <h3>Movement Is the Outcome of Design Choices</h3>
               <p>Users move through interfaces based on cues, not instructions. Layout tells them where to look. Control design tells them what can be done. Consistent terminology tells them what things mean without re-learning. Feedback tells them whether they are on track. When those parts align, the interface becomes a guide. When they conflict, the interface becomes a puzzle. The goal is not to make every experience “simple,” because some tasks are genuinely complex. The goal is to ensure that complexity belongs to the task, not to the interface. If users are spending their mental energy on navigating the system rather than accomplishing their goal, visual and interaction design have failed to support the user's movement.</p>
               <p>The most usable interfaces are not necessarily the ones with the most features or the flashiest visuals. They are the ones that respect attention, reduce needless decision-making, and make the next step feel obvious. They create trust quickly, emphasize what matters sparingly, and accept human variation gracefully. When an interface does that, users do not feel like they are operating software. They feel like they are progressing toward an outcome.</p>
            </section>

         <footer class="article-footer">
            <h3>Sources:</h3>
            <ol>
               <li>
                  Johnson, J. (2010).
                  <em>Designing with the mind in mind</em>.
                  Elsevier.
               </li>

               <li>
                  Yablonski, J. (2020).
                  <em>Laws of UX: Using psychology to design better products and services</em>.
                  O'Reilly Media.
               </li>

               <li>
                  Brodlo, W. (2023, June 30).
                  <em>UX design principle #005: Postel's law</em>.
                  Perpetual.
                  <a href="https://www.perpetualny.com/blog/ux-design-principle-005-postels-law">
                  https://www.perpetualny.com/blog/ux-design-principle-005-postels-law
                  </a>
               </li>

               <li>
                  Allman, E. (2011, August 1).
                  <em>The robustness principle reconsidered</em>.
                  <em>Communications of the ACM</em>.
                  <a href="https://cacm.acm.org/practice/the-robustness-principle-reconsidered">
                  https://cacm.acm.org/practice/the-robustness-principle-reconsidered
                  </a>
               </li>
            </ol>
         </footer>
         </article>
         <!-- Module 3.2 Blog Post Starts here -->
         <article>
            <header class="article-header">
               <h2>Module 3.2 Blog Post</h2>
            </header>
            <div class="two-col">
               <section class="col-left">
            <h3>Designing Interaction That Respects Human Time</h3>
               <p>Well-designed interaction is rarely about what a system does; it is about how it responds. Timing, motion, and feedback are not decorative layers added after functionality is complete. They are core to how users experience trust, competence, and control while interacting with a system. A button that responds instantly feels different from one that hesitates. An animation that clarifies progress feels different from one that distracts.</p>
               <p>These differences matter because interaction is emotional before it is rational. Users feel interfaces working with them or against them long before they can articulate why. Thoughtful interaction design acknowledges the limits of human perception and attention, and uses those limits as design constraints rather than obstacles. When timing, motion, and feedback are aligned with how people actually process information, the experience feels calm, predictable, and respectful. When they are misaligned, even technically correct systems can feel hostile or manipulative.</p>
            <h3>Feedback Timing</h3>
               <p>One of the most important elements of interaction design is feedback timing. Users need to know, immediately, that their action has been received. That does not mean every action must complete instantly, but it does mean that the system should acknowledge the input without delay. Johnson (2010) distinguishes responsiveness from raw performance by emphasizing that responsiveness is about meeting human time expectations, not maximizing computational speed. A system can be slow and still feel responsive if it communicates clearly, and a system can be fast and still feel frustrating if it stays silent.</p>
               <p>When feedback is delayed or ambiguous, users begin to doubt their actions. They click again. They hesitate. They wonder whether they made a mistake. This sort of uncertainty introduces cognitive load that has nothing to do with the task itself. Instead of focusing on their goal, users are forced to monitor the system's state. This erodes trust over time. The interface stops feeling like a tool and starts feeling more like something that needs to be watched carefully.</p>
               <p>On the other hand, timely feedback creates a sense of control. A button that visually responds within a fraction of a second confirms that the system is listening. A progress indicator that appears immediately, even if the task takes time, reassures users that they are not stuck. Johnson (2010) notes that responsive systems prioritize feedback according to human perceptual and cognitive deadlines, ensuring users are informed at the moments that matter most. This alignment allows users to stay engaged without anxiety, even when waiting is unavoidable.</p>
            <h3>The Doherty Threshold</h3>
               <p>The Doherty Threshold provides a useful benchmark for understanding why timing feels so critical. The principle states that productivity and engagement increase dramatically when system responses occur within approximately 400 milliseconds. Below this threshold, interactions feel seamless. Above it, users begin to perceive delay, and their attention starts to drift (Yablonski, 2020). This shift is subtle but powerful</p>
               <p>When response times exceed the Doherty Threshold, users exit what psychologists describe as a flow state (Mod, 2024). Flow occurs when attention is fully absorbed and effort feels effortless. In digital interfaces, flow depends heavily on rhythm. Each action should lead naturally to the next, without pauses that force users to reorient themselves. When a system responds too slowly or inconsistently, that rhythm breaks. Users become aware of the interface as an object rather than an extension of their intent.</p>
               <p>Designers cannot always control actual processing time, but they can control perceived responsiveness. Skeleton screens, subtle loading animations, and progress indicators can preserve flow by maintaining visual continuity. Yablonski (2020) emphasizes that animation, when used purposefully, can reduce the perception of waiting and keep users engaged during longer operations. The ethical responsibility here lies in honesty. These techniques should clarify progress, not disguise failure or manipulate patience. When users understand what is happening and why, waiting feels tolerable. When they feel misled, even short delays can provoke frustration (Mod, 2024).</p>
            <h3>Motion as Communication, Not Decoration</h3>
               <p>Motion is often misunderstood as purely aesthetic, but in interaction design, motion is a form of communication. Movement can indicate cause and effect, reveal hierarchy, and guide attention. A panel that slides into view suggests continuity. A button that compresses slightly when pressed suggests physicality and confirmation. These cues help users form mental models of how the system works.</p>
               <p>However, motion becomes harmful when it prioritizes spectacle over clarity. Jerky animations, excessive transitions, or unpredictable movement can disorient users and break focus. Johnson (2010) explicitly identifies hard-to-follow animations as a sign of poor responsiveness because they interfere with perceptual processing. Motion should support comprehension, not compete with it.</p>
               <p>Ethically designed motion respects users' cognitive limits. It moves at a pace aligned with human perception, avoids unnecessary repetition, and provides consistency across interactions. When motion is predictable, users feel safe exploring. When it is surprising without purpose, users become cautious. Over time, this caution limits engagement because users are no longer confident that the interface will behave as expected.</p>
            <h3>Jakob's Law and the Cost of Novelty</h3>
               <p>Jakob's Law explains that users spend most of their time in other products, which means they bring learned expectations into every new interface they encounter (Chung, 2023). These expectations act as a cognitive shortcut. When an interface behaves as users anticipate, they can stay focused on their goal. When it does not, attention shifts away from the task and toward figuring out the interface itself.</p>
               <p>This shift has emotional and ethical consequences. When familiar interaction patterns are replaced with unnecessary novelty, users must relearn basic behaviors such as navigation, feedback cues, or error handling. The cost is not only confusion, but mental fatigue. Users move from goal focused behavior into interface monitoring, which interrupts flow and increases cognitive load (Chung, 2023). Over time, this makes an experience feel demanding rather than supportive.</p>
               <p>Jakob's Law does not argue against innovation. Instead, it asks designers to be deliberate about where novelty is introduced. Familiar timing, predictable motion, and standard interaction patterns allow users to conserve attention for moments that truly matter. In this sense, familiarity becomes an ethical design choice. It respects the user's time, energy, and prior knowledge rather than forcing them to adapt to avoidable friction.</p>
               <p>When systems align with established patterns, feedback feels trustworthy and motion feels meaningful. When they do not, even well-intentioned design decisions can feel careless or self-indulgent. Designing with Jakob's Law in mind reinforces the broader goal of ethical interaction design by prioritizing clarity over cleverness and respect over reinvention.</p>
               <h3>Error Prevention and Designing for Recovery</h3>
               <p>Errors are inevitable, but frustration is optional. Ethical interaction design anticipates mistakes and minimizes their consequences. Error prevention begins with clear affordances and timely feedback, but it also extends to how systems respond when something goes wrong. A well-designed error state explains what happened, why it happened, and what the user can do next.</p>
               <p>Timing plays a critical role here. Errors that appear only after submission, without prior guidance, feel punitive. Users are forced into a loop of guesswork and correction. In contrast, systems that provide inline validation or gentle warnings before errors occur feel collaborative. They help users succeed rather than catching them failing. This approach aligns with Johnson's (2010) emphasis on keeping users informed and reducing uncertainty throughout an interaction.</p>
               <p>Ethical considerations become especially important when errors intersect with persuasive design. Dark patterns often exploit timing and feedback to pressure users into decisions, such as delaying the appearance of a cancel option or animating confirmations in misleading ways. These techniques may increase short-term conversions, but they undermine long-term trust. A system that relies on confusion to succeed is not respecting the user's autonomy.</p>
            </section>
               <aside class="col-right">
               <h3>The Peak-End Rule and Lasting Impressions</h3>
               <p>The Peak-End Rule reminds designers that users remember experiences based largely on emotional peaks and final moments, not on the average quality of every step (Yablonski, 2020). This has profound implications for interaction design. A smooth onboarding followed by a confusing error can overshadow everything that came before. Likewise, a frustrating process followed by a clear, reassuring conclusion can redeem the experience.</p>
               <p>Timing and motion play an outsized role in these moments. The peak of an experience might be a moment of success, relief, or frustration. The end might be a confirmation screen, a receipt, or a final message. If these moments feel rushed, ambiguous, or manipulative, they dominate memory in negative ways. If they feel thoughtful and respectful, users are more likely to forgive minor friction elsewhere.</p>
               <p>Ethical design requires intentionality at these moments. Designers should ask not only whether users can complete a task, but how they will feel when it ends. A calm confirmation, clear next steps, and honest feedback contribute to positive recall. Yablonski (2020) notes that people remember negative experiences more vividly than positive ones, which places an even greater responsibility on designers to avoid unnecessary emotional spikes.</p>
            <h3>A Personal Experience With Poor Interaction Timing</h3>
               <p>One of the most frustrating digital experiences I have had recently involved a financial dashboard that updated account balances asynchronously without clear feedback. After initiating a transfer, the interface froze momentarily, then refreshed without confirmation. There was no animation, no message, and no indication of whether the action had succeeded. I waited, refreshed the page manually, and ultimately checked my email for reassurance.</p>
               <p>The system likely completed the transaction correctly, but the lack of timely feedback transformed a routine action into a stressful one. The issue was not performance but communication. Johnson's (2010) analogy of the watch repairman who works silently applies perfectly here. Without acknowledgment, users are left guessing. That guessing erodes confidence, even in systems that are technically reliable.</p>
               <p>In contrast, I have used other financial apps that take slightly longer to process the same action but provide immediate confirmation, progress indicators, and clear end states. Despite being slower, these systems feel more trustworthy. The difference lies entirely in interaction design, not functionality.</p>
            <h3>Ethics, Persuasion, and Respecting Attention</h3>
               <p>Persuasive design is not inherently unethical. Encouraging users to complete beneficial actions, stay engaged, or explore features can be aligned with user goals. The ethical line is crossed when persuasion relies on obscuring information, manipulating timing, or exploiting cognitive biases without transparency. Delaying negative feedback, hiding opt-out paths, or using motion to rush decisions undermines informed consent.</p>
               <p>Designers have a responsibility to recognize the power they hold over attention and emotion. The same principles that make interfaces feel delightful can also make them coercive. Ethical interaction design uses timing, motion, and feedback to support understanding, not to bypass it. It respects the user's ability to decide rather than attempting to outmaneuver it.</p>
               <p>When systems are responsive, predictable, and honest, users feel empowered. They move through tasks with confidence instead of suspicion. They are more likely to return, recommend the product, and trust the organization behind it. These outcomes are not just morally preferable; they are also sustainable.</p>
            <h3>Designing Interactions That Feel Human</h3>
               <p>Ultimately, interaction design succeeds when it aligns with human rhythms. The brain operates on multiple time scales, and systems that respect those scales feel intuitive. Immediate acknowledgment, smooth motion, clear recovery paths, and thoughtful endings work together to create experiences that feel humane rather than mechanical. Johnson (2010) reminds us that responsiveness is about compliance with human time requirements, not technical benchmarks. Yablonski (2020) reminds us that memory and emotion shape how experiences are judged long after they end.</p>
               <p>When designers take these insights seriously, interaction patterns become more than patterns. They become expressions of respect. Timing stops feeling rushed or sluggish. Motion stops feeling ornamental. Error handling stops feeling punitive. The interface becomes a partner in the task, guiding users forward without pressure or deception. Meaningful and ethical user experiences are built in these details. They are not loud or flashy. They are quiet, consistent, and trustworthy. And when they are done well, users do not notice the design at all. They simply feel that the system understood them.</p>
            
            </aside>
            </div>
             <section class="image-bar" aria-label="Image bar">

               <figure>
                  <img src="images/design interactions.jpg" alt="Laptop displaying interface design layouts and interaction elements on screen">
                  <figcaption>
                     Photo by
                     <a href="https://unsplash.com/@tirzavandijk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                     Tirza van Dijk
                     </a>
                     on
                     <a href="https://unsplash.com/photos/macbook-pro-displaying-computer-icons-o1SKqmgSDbg?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                     Unsplash
                     </a>.
                  </figcaption>
               </figure>

               <figure>
                  <img src="images/interaction.jpg" alt="Person holding a smartphone and interacting with a mobile application">
                  <figcaption>
                     Photo by
                     <a href="https://unsplash.com/@georgiadelotz?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                     Georgia de Lotz
                     </a>
                     on
                     <a href="https://unsplash.com/photos/person-holding-black-android-smartphone-zG9bNfiUeCQ?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                     Unsplash
                     </a>.
                  </figcaption>
               </figure>

               <figure>
                  <img src="images/motion.jpg" alt="Designer working on a web application interface with visual interaction elements">
                  <figcaption>
                     Photo by
                     <a href="https://unsplash.com/@tirzavandijk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                     Tirza van Dijk
                     </a>
                     on
                     <a href="https://unsplash.com/photos/macbook-pro-displaying-computer-icons-o1SKqmgSDbg?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">
                     Unsplash
                     </a>.
                  </figcaption>
               </figure>

            </section>   
         <footer class="article-footer">
            <h3>Sources:</h3>
            <ol>
               <li>
                  Johnson, J. (2010).
                  <em>Designing with the mind in mind</em>.
                  Elsevier.
               </li>
               <li>
                  Yablonski, J. (2020).
                  <em>Laws of UX: Using psychology to design better products and services</em>.
                  O'Reilly Media.
               </li>
               <li>
                  Mod, D. (2024, March 29).
                  <em>Doherty Threshold: UX law of swift interactions</em>.
                  UXtweak.
                  <a href="https://blog.uxtweak.com/doherty-threshold/">
                  https://blog.uxtweak.com/doherty-threshold/
                  </a>
               </li>
               <li>
                  Chung, E. (2023, June 22).
                  <em>Jakob's Law: Creating familiar and user-centric interfaces</em>.
                  LogRocket.
                  <a href="https://blog.logrocket.com/ux-design/jakobs-law-creating-user-centric-interfaces">
                  https://blog.logrocket.com/ux-design/jakobs-law-creating-user-centric-interfaces
                  </a>
               </li>
            </ol>
         </footer>
         </article>
      </main>
<!-- Page Footer  -->
      <footer class="page-footer">
       <p>Copyright &copy; 2025</p>
      </footer>
   </body>
</html>